{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e4f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01fd8b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7043, 21)\n"
     ]
    }
   ],
   "source": [
    "data_1 = pd.read_csv(\"Data/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "\n",
    "print(data_1.shape)\n",
    "\n",
    "data_1 = data_1.replace(\"?\", data_1.mode().loc[0])\n",
    "data_1 = data_1.fillna(0)\n",
    "\n",
    "# One Hot Encoding\n",
    "data_1 = pd.get_dummies(data_1, columns=['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod'])\n",
    "\n",
    "# Lebel Encoding\n",
    "data_1 = pd.get_dummies(data_1, columns=['gender', 'Partner', 'Dependents', 'PhoneService','PaperlessBilling'])\n",
    "\n",
    "\n",
    "data_1 = data_1.drop(['gender_Female'], axis=1)\n",
    "data_1 = data_1.rename(columns={\"gender_Male\": \"Gender\"})\n",
    "\n",
    "data_1 = data_1.drop(['Partner_No'], axis=1)\n",
    "data_1 = data_1.rename(columns={\"Partner_Yes\": \"Partner\"})\n",
    "\n",
    "data_1 = data_1.drop(['Dependents_No'], axis=1)\n",
    "data_1 = data_1.rename(columns={\"Dependents_Yes\": \"Dependents\"})\n",
    "\n",
    "data_1 = data_1.drop(['PhoneService_No'], axis=1)\n",
    "data_1 = data_1.rename(columns={\"PhoneService_Yes\": \"PhoneService\"})\n",
    "\n",
    "data_1 = data_1.drop(['PaperlessBilling_No'], axis=1)\n",
    "data_1 = data_1.rename(columns={\"PaperlessBilling_Yes\": \"PaperlessBilling\"})\n",
    "\n",
    "data_1['TotalCharges'] = pd.to_numeric(data_1['TotalCharges'], errors='coerce')\n",
    "#data_1['TotalCharges'] = data_1['TotalCharges'].astype(float) \n",
    "\n",
    "data_1 = data_1.fillna(0)\n",
    "\n",
    "#Normalization\n",
    "data_1['MonthlyCharges'] = (data_1['MonthlyCharges'] - data_1['MonthlyCharges'].min()) / (data_1['MonthlyCharges'].max()-data_1['MonthlyCharges'].min())\n",
    "data_1['TotalCharges'] = (data_1['TotalCharges'] - data_1['TotalCharges'].min()) / (data_1['TotalCharges'].max()-data_1['TotalCharges'].min()) \n",
    "\n",
    "data_1[\"Churn\"] = data_1[\"Churn\"].replace(\"Yes\", 1)\n",
    "data_1[\"Churn\"] = data_1[\"Churn\"].replace(\"No\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5db71839",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'nuinique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5470/420203154.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnuinique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'nuinique'"
     ]
    }
   ],
   "source": [
    "data_1.nuinique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "316b12b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    5174\n",
       " 1    1869\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1['Churn'].value_counts()\n",
    "#data_1.dtypes\n",
    "#pd.notnull(data_1[\"TotalCharges\"]).value_counts()\n",
    "#data_1['Gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "973cc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(data_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542a58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858833f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88b612d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5567ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', '>50K']\n",
    "data_2 = pd.read_csv(\"Data/adult/adultData.csv\", names = columns, skipinitialspace = True)\n",
    "\n",
    "#data_2 = data_2.replace(r\"?\", data_2.mode().loc[0], regex=True)\n",
    "data_2 = data_2.replace(\"?\", data_2.mode().loc[0])\n",
    "data_2 = data_2.fillna(0)\n",
    "\n",
    "# One Hot Encoding\n",
    "data_2 = pd.get_dummies(data_2, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country'])\n",
    "\n",
    "# Label Encoding\n",
    "data_2 = pd.get_dummies(data_2, columns=['sex'])\n",
    "\n",
    "data_2 = data_2.drop(['sex_Female'], axis=1)\n",
    "data_2 = data_2.rename(columns={\"sex_Male\": \"sex\"})\n",
    "\n",
    "data_2['>50K'] = data_2['>50K'].replace('>50K', 1)\n",
    "data_2['>50K'] = data_2['>50K'].replace('<=50K', -1)\n",
    "\n",
    "\n",
    "\n",
    "# Normalization\n",
    "df = data_2[['fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']]\n",
    "normalized_df = (df - df.min())/(df.max()-df.min())\n",
    "\n",
    "data_2[['fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']] = normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b812c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_2.dtypes\n",
    "#data_2.nunique(axis=0)\n",
    "\n",
    "#pd.isnull(data_2).nunique(axis=0)\n",
    "#pd.notnull(data_2).nunique(axis=0)\n",
    "#data_2.notnull().value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44afd9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(data_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "420fb065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    24720\n",
       " 1     7841\n",
       "Name: >50K, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2['>50K'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518daea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff146a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06019da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = pd.read_csv(\"Data/creditcard.csv\", skipinitialspace = True)\n",
    "\n",
    "data_3 = data_3.replace(\"?\", data_3.mean())\n",
    "data_3 = data_3.fillna(0)\n",
    "\n",
    "#Normalization\n",
    "df = data_3[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]\n",
    "\n",
    "normalized_df = (df - df.min()) / (df.max()-df.min())\n",
    "data_3[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']] = normalized_df\n",
    "\n",
    "data_3['Class'] = data_3['Class'].replace(0, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21153701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    284315\n",
       " 1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_3.dtypes\n",
    "#data_3.nunique(axis=0)\n",
    "\n",
    "#pd.isnull(data_3).nunique(axis=0)\n",
    "#pd.notnull(data_3).nunique(axis=0)\n",
    "#data_3.notnull().value_counts()\n",
    "data_3['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a2ed8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(data_3.columns)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "6cf97b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b221b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d123cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e1cb443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500745</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396423</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347243</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.418778</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.198582</td>\n",
       "      <td>0.642325</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.507538</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.490313</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.613065</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548435</td>\n",
       "      <td>0.111870</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.608040</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.132388</td>\n",
       "      <td>0.390462</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.633166</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448584</td>\n",
       "      <td>0.115713</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.467337</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453055</td>\n",
       "      <td>0.101196</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           v1        v2        v3        v4        v5        v6        v7  \\\n",
       "0    0.352941  0.743719  0.590164  0.353535  0.000000  0.500745  0.234415   \n",
       "1    0.058824  0.427136  0.540984  0.292929  0.000000  0.396423  0.116567   \n",
       "2    0.470588  0.919598  0.524590  0.000000  0.000000  0.347243  0.253629   \n",
       "3    0.058824  0.447236  0.540984  0.232323  0.111111  0.418778  0.038002   \n",
       "4    0.000000  0.688442  0.327869  0.353535  0.198582  0.642325  0.943638   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "763  0.588235  0.507538  0.622951  0.484848  0.212766  0.490313  0.039710   \n",
       "764  0.117647  0.613065  0.573770  0.272727  0.000000  0.548435  0.111870   \n",
       "765  0.294118  0.608040  0.590164  0.232323  0.132388  0.390462  0.071307   \n",
       "766  0.058824  0.633166  0.491803  0.000000  0.000000  0.448584  0.115713   \n",
       "767  0.058824  0.467337  0.573770  0.313131  0.000000  0.453055  0.101196   \n",
       "\n",
       "           v8  class  \n",
       "0    0.483333      1  \n",
       "1    0.166667     -1  \n",
       "2    0.183333      1  \n",
       "3    0.000000     -1  \n",
       "4    0.200000      1  \n",
       "..        ...    ...  \n",
       "763  0.700000     -1  \n",
       "764  0.100000     -1  \n",
       "765  0.150000     -1  \n",
       "766  0.433333      1  \n",
       "767  0.033333     -1  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\",\"v6\", \"v7\", \"v8\", \"class\"]\n",
    "data_4 = data_3 = pd.read_csv(\"Data/data.csv\", names = columns, skipinitialspace = True)\n",
    "\n",
    "#Normalization\n",
    "df = data_4[['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8']]\n",
    "\n",
    "normalized_df = (df - df.min()) / (df.max()-df.min())\n",
    "data_4[['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8']] = normalized_df\n",
    "\n",
    "data_4['class'] = data_4['class'].replace(0, -1) \n",
    "data_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aed002bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    500\n",
       " 1    268\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_4.dtypes\n",
    "#data_4.nunique(axis=0)\n",
    "#data_4['class'].value_counts()\n",
    "#data_4.notnull().nunique(axis=0)\n",
    "data_4['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0211a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d39dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "92624d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "features = ['SeniorCitizen',\n",
    " 'tenure',\n",
    " 'MonthlyCharges',\n",
    " 'TotalCharges',\n",
    " 'MultipleLines_No',\n",
    " 'MultipleLines_No phone service',\n",
    " 'MultipleLines_Yes',\n",
    " 'InternetService_DSL',\n",
    " 'InternetService_Fiber optic',\n",
    " 'InternetService_No',\n",
    " 'OnlineSecurity_No',\n",
    " 'OnlineSecurity_No internet service',\n",
    " 'OnlineSecurity_Yes',\n",
    " 'OnlineBackup_No',\n",
    " 'OnlineBackup_No internet service',\n",
    " 'OnlineBackup_Yes',\n",
    " 'DeviceProtection_No',\n",
    " 'DeviceProtection_No internet service',\n",
    " 'DeviceProtection_Yes',\n",
    " 'TechSupport_No',\n",
    " 'TechSupport_No internet service',\n",
    " 'TechSupport_Yes',\n",
    " 'StreamingTV_No',\n",
    " 'StreamingTV_No internet service',\n",
    " 'StreamingTV_Yes',\n",
    " 'StreamingMovies_No',\n",
    " 'StreamingMovies_No internet service',\n",
    " 'StreamingMovies_Yes',\n",
    " 'Contract_Month-to-month',\n",
    " 'Contract_One year',\n",
    " 'Contract_Two year',\n",
    " 'PaymentMethod_Bank transfer (automatic)',\n",
    " 'PaymentMethod_Credit card (automatic)',\n",
    " 'PaymentMethod_Electronic check',\n",
    " 'PaymentMethod_Mailed check',\n",
    " 'Gender',\n",
    " 'Partner',\n",
    " 'Dependents',\n",
    " 'PhoneService',\n",
    " 'PaperlessBilling']\n",
    "X = data_1[features]\n",
    "y = data_1.Churn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "39309a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age',\n",
    " 'fnlwgt',\n",
    " 'education-num',\n",
    " 'capital-gain',\n",
    " 'capital-loss',\n",
    " 'hours-per-week',\n",
    " 'workclass_Federal-gov',\n",
    " 'workclass_Local-gov',\n",
    " 'workclass_Never-worked',\n",
    " 'workclass_Private',\n",
    " 'workclass_Self-emp-inc',\n",
    " 'workclass_Self-emp-not-inc',\n",
    " 'workclass_State-gov',\n",
    " 'workclass_Without-pay',\n",
    " 'education_10th',\n",
    " 'education_11th',\n",
    " 'education_12th',\n",
    " 'education_1st-4th',\n",
    " 'education_5th-6th',\n",
    " 'education_7th-8th',\n",
    " 'education_9th',\n",
    " 'education_Assoc-acdm',\n",
    " 'education_Assoc-voc',\n",
    " 'education_Bachelors',\n",
    " 'education_Doctorate',\n",
    " 'education_HS-grad',\n",
    " 'education_Masters',\n",
    " 'education_Preschool',\n",
    " 'education_Prof-school',\n",
    " 'education_Some-college',\n",
    " 'marital-status_Divorced',\n",
    " 'marital-status_Married-AF-spouse',\n",
    " 'marital-status_Married-civ-spouse',\n",
    " 'marital-status_Married-spouse-absent',\n",
    " 'marital-status_Never-married',\n",
    " 'marital-status_Separated',\n",
    " 'marital-status_Widowed',\n",
    " 'occupation_Adm-clerical',\n",
    " 'occupation_Armed-Forces',\n",
    " 'occupation_Craft-repair',\n",
    " 'occupation_Exec-managerial',\n",
    " 'occupation_Farming-fishing',\n",
    " 'occupation_Handlers-cleaners',\n",
    " 'occupation_Machine-op-inspct',\n",
    " 'occupation_Other-service',\n",
    " 'occupation_Priv-house-serv',\n",
    " 'occupation_Prof-specialty',\n",
    " 'occupation_Protective-serv',\n",
    " 'occupation_Sales',\n",
    " 'occupation_Tech-support',\n",
    " 'occupation_Transport-moving',\n",
    " 'relationship_Husband',\n",
    " 'relationship_Not-in-family',\n",
    " 'relationship_Other-relative',\n",
    " 'relationship_Own-child',\n",
    " 'relationship_Unmarried',\n",
    " 'relationship_Wife',\n",
    " 'race_Amer-Indian-Eskimo',\n",
    " 'race_Asian-Pac-Islander',\n",
    " 'race_Black',\n",
    " 'race_Other',\n",
    " 'race_White',\n",
    " 'native-country_Cambodia',\n",
    " 'native-country_Canada',\n",
    " 'native-country_China',\n",
    " 'native-country_Columbia',\n",
    " 'native-country_Cuba',\n",
    " 'native-country_Dominican-Republic',\n",
    " 'native-country_Ecuador',\n",
    " 'native-country_El-Salvador',\n",
    " 'native-country_England',\n",
    " 'native-country_France',\n",
    " 'native-country_Germany',\n",
    " 'native-country_Greece',\n",
    " 'native-country_Guatemala',\n",
    " 'native-country_Haiti',\n",
    " 'native-country_Holand-Netherlands',\n",
    " 'native-country_Honduras',\n",
    " 'native-country_Hong',\n",
    " 'native-country_Hungary',\n",
    " 'native-country_India',\n",
    " 'native-country_Iran',\n",
    " 'native-country_Ireland',\n",
    " 'native-country_Italy',\n",
    " 'native-country_Jamaica',\n",
    " 'native-country_Japan',\n",
    " 'native-country_Laos',\n",
    " 'native-country_Mexico',\n",
    " 'native-country_Nicaragua',\n",
    " 'native-country_Outlying-US(Guam-USVI-etc)',\n",
    " 'native-country_Peru',\n",
    " 'native-country_Philippines',\n",
    " 'native-country_Poland',\n",
    " 'native-country_Portugal',\n",
    " 'native-country_Puerto-Rico',\n",
    " 'native-country_Scotland',\n",
    " 'native-country_South',\n",
    " 'native-country_Taiwan',\n",
    " 'native-country_Thailand',\n",
    " 'native-country_Trinadad&Tobago',\n",
    " 'native-country_United-States',\n",
    " 'native-country_Vietnam',\n",
    " 'native-country_Yugoslavia',\n",
    " 'sex']\n",
    "\n",
    "X = data_2[features]\n",
    "y = data_2['>50K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cd990d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['V1',\n",
    " 'V2',\n",
    " 'V3',\n",
    " 'V4',\n",
    " 'V5',\n",
    " 'V6',\n",
    " 'V7',\n",
    " 'V8',\n",
    " 'V9',\n",
    " 'V10',\n",
    " 'V11',\n",
    " 'V12',\n",
    " 'V13',\n",
    " 'V14',\n",
    " 'V15',\n",
    " 'V16',\n",
    " 'V17',\n",
    " 'V18',\n",
    " 'V19',\n",
    " 'V20',\n",
    " 'V21',\n",
    " 'V22',\n",
    " 'V23',\n",
    " 'V24',\n",
    " 'V25',\n",
    " 'V26',\n",
    " 'V27',\n",
    " 'V28',\n",
    " 'Amount']\n",
    "X = data_3[features]\n",
    "y = data_3.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2644d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9baa5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20008f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pronob/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred=logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd64691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[935 106]\n",
      " [178 190]]\n",
      "0.7984386089425124\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cnf_matrix)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d7885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b866dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_names=[0,1] # name  of classes\n",
    "#fig, ax = plt.subplots()\n",
    "#tick_marks = np.arange(len(class_names))\n",
    "#plt.xticks(tick_marks, class_names)\n",
    "#plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "#sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "#ax.xaxis.set_label_position(\"top\")\n",
    "#plt.tight_layout()\n",
    "#plt.title('Confusion matrix', y=1.1)\n",
    "#plt.ylabel('Actual label')\n",
    "#plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7ca2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148d799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855d138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20017c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dc0f9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate = 0.00024, n_iterations = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = 0\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, loss_threshold):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros((n_features,1))\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            linear = np.dot(X, self.weights) #+ self.bias X.dot(self.weights) #\n",
    "            y_hat = np.tanh(linear)\n",
    "            \n",
    "            y_predicted = self.predict(X)\n",
    "            accuracy = metrics.accuracy_score(y, y_predicted)\n",
    "            print(accuracy)\n",
    "            \n",
    "            if (1-accuracy) < loss_threshold:\n",
    "                break\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (y - y_hat)*(1-y_hat**2))\n",
    "            self.weights +=  self.learning_rate * dw\n",
    "        return self.weights\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear = np.dot(X, self.weights)\n",
    "        y_hat = np.tanh(linear)\n",
    "        predicted_cls = [1 if i > 0 else -1 for i in y_hat]\n",
    "        return predicted_cls\n",
    "    \n",
    "    \n",
    "    def _tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "    \n",
    "        \n",
    "    def _mse(self, y, h_hat):\n",
    "        mse = np.sum((y - y_hat)**2, axis=1)/n_samples\n",
    "        return mse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "253eb847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9982839210867037\n",
      "56962\n",
      "227845\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "\n",
    "y_train = y_train.to_numpy().reshape((y_train.shape[0], 1))\n",
    "X_train = X_train.to_numpy()\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis = 1)\n",
    "\n",
    "y_test = y_test.to_numpy().reshape((y_test.shape[0], 1))\n",
    "X_test = X_test.to_numpy()\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis = 1)\n",
    "\n",
    "mylogreg = MyLogisticRegression()\n",
    "w = mylogreg.fit(X_train,y_train, 0.5)\n",
    "\n",
    "print(y_test.size)\n",
    "print(y_train.size)\n",
    "#y_hat = mylogreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b5ed18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = mylogreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68467610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c3c03654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56861     0]\n",
      " [  101     0]]\n",
      "0.9982268881008391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00     56861\n",
      "           1       0.00      0.00      0.00       101\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.50      0.50      0.50     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pronob/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pronob/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pronob/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_hat)\n",
    "print(cnf_matrix)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_hat))\n",
    "print(metrics.classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceba671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69495303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "90dc6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaBoast(X, y, L_weak, K):\n",
    "    examples = np.concatenate((X,y), axis = 1)\n",
    "    #print(\"ex shape\", examples.shape)\n",
    "    w = [] #np.ones(X.shape[0])*(1/X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        w.append(1/X.shape[0])\n",
    "    \n",
    "    h = []\n",
    "    z = np.zeros(K)\n",
    "    \n",
    "    #print(\"w\", len(w))\n",
    "    for k in range(K):\n",
    "        data = Resample(examples, w)\n",
    "        X1 = data.iloc[ : , :-1]\n",
    "        y1 = data[data.columns[-1]]\n",
    "        \n",
    "        #print(X1.shape, y1.shape)\n",
    "        #X1 = X1.to_numpy()\n",
    "        #y1 = y1.to_numpy()\n",
    "        #print(X1.shape, y1.shape)\n",
    "        h.append(L_weak.fit(X1, y1, 0.5))\n",
    "        #print(\"h\", h)\n",
    "        error = 0\n",
    "        \n",
    "        for j in range(examples.shape[0]):\n",
    "            #print(\"last\", h[k])\n",
    "            y_hat = np.tanh(np.dot(X, h[k]))\n",
    "            predicted_cls = [1 if i > 0 else -1 for i in y_hat]\n",
    "            \n",
    "            if  predicted_cls[j] != y[j]:\n",
    "                error += w[j] \n",
    "            \n",
    "        if error > 0.5:\n",
    "            continue\n",
    "            \n",
    "        for j in range(examples.shape[0]):\n",
    "            y_hat = np.tanh(np.dot(X, h[k]))\n",
    "            predicted_cls = [1 if i > 0 else -1 for i in y_hat]\n",
    "            \n",
    "            if  predicted_cls[j] == y[j]:\n",
    "                w[j] = w[j]* error / (1 - error)\n",
    "        \n",
    "        for i in range(len(w)):\n",
    "            w[i] = w[i]/np.sum(w)\n",
    "        \n",
    "        z[k] = np.log2((1-error)/error)\n",
    "        \n",
    "        return (h,z) #Weighted_Majority(X, h, z)        \n",
    "        \n",
    "        \n",
    "        \n",
    "def Resample(examples, w):\n",
    "    \n",
    "    df = []\n",
    "    #print(\"Re\", examples.shape)\n",
    "    index = np.random.choice(examples.shape[0], examples.shape[0], p=w)\n",
    "    for i in index:\n",
    "        df.append(examples[i])\n",
    "    df = pd.DataFrame(df)\n",
    "    #print(df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def Weighted_Majority(X, h, z):\n",
    "    sum = 0\n",
    "    for i in range(len(h)):\n",
    "        sum += np.tanh(np.dot(X, h[i])) * z[i]\n",
    "    return sum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "22c8c8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_t shape (5634, 41)\n",
      "y_train shape (5634, 1)\n",
      "0.7341143059992901\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "\n",
    "y_train = y_train.to_numpy().reshape((y_train.shape[0], 1))\n",
    "X_train = X_train.to_numpy()\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis = 1)\n",
    "\n",
    "y_test = y_test.to_numpy().reshape((y_test.shape[0], 1))\n",
    "X_test = X_test.to_numpy()\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis = 1)\n",
    "\n",
    "mylogreg = MyLogisticRegression()\n",
    "#mylogreg.fit(X_train,y_train, 0.5)\n",
    "\n",
    "#print(y_test.size)\n",
    "#print(y_train.size)\n",
    "\n",
    "print(\"X_t shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "(h,z) = adaBoast( X_train, y_train, mylogreg, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e8456f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4133    0]\n",
      " [1501    0]]\n",
      "0.7335818246361377\n"
     ]
    }
   ],
   "source": [
    "y_hat = Weighted_Majority(X_train, h, z)\n",
    "predicted_cls = [1 if i > 0 else -1 for i in y_hat]\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_train, predicted_cls)\n",
    "print(cnf_matrix)\n",
    "\n",
    "print(metrics.accuracy_score(y_train, predicted_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28fd421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a8818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bdfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21c718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ab5839e",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d038bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Confussion MAtrix:\n",
      " [[107   0]\n",
      " [ 47   0]]\n",
      "Accuracy: 0.6948051948051948\n",
      "AdaBoost:\n",
      "Confussion Matrix:\n",
      " [[107   0]\n",
      " [ 47   0]]\n",
      "Accuracy: 0.6948051948051948\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Loading Dataset 1\n",
    "def read_data1(path):\n",
    "\tdata_1 = pd.read_csv(path)\n",
    "\n",
    "\tdata_1 = data_1.replace(\"?\", data_1.mode().loc[0])\n",
    "\tdata_1 = data_1.fillna(0)\n",
    "\n",
    "\t# One Hot Encoding\n",
    "\tdata_1 = pd.get_dummies(data_1, columns=['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod'])\n",
    "\n",
    "\t# Lebel Encoding\n",
    "\tdata_1 = pd.get_dummies(data_1, columns=['gender', 'Partner', 'Dependents', 'PhoneService','PaperlessBilling'])\n",
    "\n",
    "\tdata_1 = data_1.drop(['gender_Female'], axis=1)\n",
    "\tdata_1 = data_1.rename(columns={\"gender_Male\": \"Gender\"})\n",
    "\n",
    "\tdata_1 = data_1.drop(['Partner_No'], axis=1)\n",
    "\tdata_1 = data_1.rename(columns={\"Partner_Yes\": \"Partner\"})\n",
    "\n",
    "\tdata_1 = data_1.drop(['Dependents_No'], axis=1)\n",
    "\tdata_1 = data_1.rename(columns={\"Dependents_Yes\": \"Dependents\"})\n",
    "\n",
    "\tdata_1 = data_1.drop(['PhoneService_No'], axis=1)\n",
    "\tdata_1 = data_1.rename(columns={\"PhoneService_Yes\": \"PhoneService\"})\n",
    "\n",
    "\tdata_1 = data_1.drop(['PaperlessBilling_No'], axis=1)\n",
    "\tdata_1 = data_1.rename(columns={\"PaperlessBilling_Yes\": \"PaperlessBilling\"})\n",
    "\n",
    "\tdata_1['TotalCharges'] = pd.to_numeric(data_1['TotalCharges'], errors='coerce')\n",
    "\t#data_1['TotalCharges'] = data_1['TotalCharges'].astype(float) \n",
    "\n",
    "\tdata_1 = data_1.fillna(0)\n",
    "\n",
    "\t#Normalization\n",
    "\tdata_1['MonthlyCharges'] = (data_1['MonthlyCharges'] - data_1['MonthlyCharges'].min()) / (data_1['MonthlyCharges'].max()-data_1['MonthlyCharges'].min())\n",
    "\tdata_1['TotalCharges'] = (data_1['TotalCharges'] - data_1['TotalCharges'].min()) / (data_1['TotalCharges'].max()-data_1['TotalCharges'].min()) \n",
    "\t\n",
    "\tdata_1[\"Churn\"] = data_1[\"Churn\"].replace(\"Yes\", 1)\n",
    "\tdata_1[\"Churn\"] = data_1[\"Churn\"].replace(\"No\", -1)\n",
    "\t\n",
    "\treturn data_1\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "# Loading Dataset 2\n",
    "def read_data2(path):\t\n",
    "\tcolumns=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', '>50K']\n",
    "\tdata_2 = pd.read_csv(path, names = columns, skipinitialspace = True)\n",
    "\n",
    "\t#data_2 = data_2.replace(r\"?\", data_2.mode().loc[0], regex=True)\n",
    "\tdata_2 = data_2.replace(\"?\", data_2.mode().loc[0])\n",
    "\tdata_2 = data_2.fillna(0)\n",
    "\n",
    "\t# One Hot Encoding\n",
    "\tdata_2 = pd.get_dummies(data_2, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country'])\n",
    "\n",
    "\t# Label Encoding\n",
    "\tdata_2 = pd.get_dummies(data_2, columns=['sex'])\n",
    "\n",
    "\tdata_2 = data_2.drop(['sex_Female'], axis=1)\n",
    "\tdata_2 = data_2.rename(columns={\"sex_Male\": \"sex\"})\n",
    "\n",
    "\tdata_2['>50K'] = data_2['>50K'].replace('>50K', 1)\n",
    "\tdata_2['>50K'] = data_2['>50K'].replace('<=50K', -1)\n",
    "\n",
    "\n",
    "\t# Normalization\n",
    "\tdf = data_2[['fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']]\n",
    "\tnormalized_df = (df - df.min())/(df.max()-df.min())\n",
    "\n",
    "\tdata_2[['fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']] = normalized_df\n",
    "\n",
    "\treturn data_2\n",
    "\n",
    "\n",
    "\n",
    "# Loading Dataset 3\n",
    "def read_data3(path):\n",
    "\tdata_3 = pd.read_csv(\"path\", skipinitialspace = True)\n",
    "\n",
    "\tdata_3 = data_3.replace(\"?\", data_3.mean())\n",
    "\tdata_3 = data_3.fillna(0)\n",
    "\n",
    "\t#Normalization\n",
    "\tdf = data_3[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]\n",
    "\n",
    "\tnormalized_df = (df - df.min()) / (df.max()-df.min())\n",
    "\tdata_3[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']] = normalized_df\n",
    "\n",
    "\tdata_3['Class'] = data_3['Class'].replace(0, -1) \n",
    "\treturn data_3\n",
    "\t\n",
    "\t\n",
    "    \n",
    "def read_data4(path):\n",
    "    columns = [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\",\"v6\", \"v7\", \"v8\", \"class\"]\n",
    "    data_4 = data_3 = pd.read_csv(path, names = columns, skipinitialspace = True)\n",
    "\n",
    "    #Normalization\n",
    "    df = data_4[['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8']]\n",
    "\n",
    "    normalized_df = (df - df.min()) / (df.max()-df.min())\n",
    "    data_4[['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8']] = normalized_df\n",
    "\n",
    "    data_4['class'] = data_4['class'].replace(0, -1) \n",
    "    return data_4\n",
    "\n",
    "\n",
    "path = \"Data/data.csv\"\n",
    "\n",
    "data_4 = read_data4(path)\n",
    "# give var name \"data_1\" for first dataset\n",
    "# give var name data_2 for second dataset\n",
    "# give var name data_3 for third dataset\n",
    "\n",
    "\n",
    "features1 = ['SeniorCitizen',\n",
    " 'tenure',\n",
    " 'MonthlyCharges',\n",
    " 'TotalCharges',\n",
    " 'MultipleLines_No',\n",
    " 'MultipleLines_No phone service',\n",
    " 'MultipleLines_Yes',\n",
    " 'InternetService_DSL',\n",
    " 'InternetService_Fiber optic',\n",
    " 'InternetService_No',\n",
    " 'OnlineSecurity_No',\n",
    " 'OnlineSecurity_No internet service',\n",
    " 'OnlineSecurity_Yes',\n",
    " 'OnlineBackup_No',\n",
    " 'OnlineBackup_No internet service',\n",
    " 'OnlineBackup_Yes',\n",
    " 'DeviceProtection_No',\n",
    " 'DeviceProtection_No internet service',\n",
    " 'DeviceProtection_Yes',\n",
    " 'TechSupport_No',\n",
    " 'TechSupport_No internet service',\n",
    " 'TechSupport_Yes',\n",
    " 'StreamingTV_No',\n",
    " 'StreamingTV_No internet service',\n",
    " 'StreamingTV_Yes',\n",
    " 'StreamingMovies_No',\n",
    " 'StreamingMovies_No internet service',\n",
    " 'StreamingMovies_Yes',\n",
    " 'Contract_Month-to-month',\n",
    " 'Contract_One year',\n",
    " 'Contract_Two year',\n",
    " 'PaymentMethod_Bank transfer (automatic)',\n",
    " 'PaymentMethod_Credit card (automatic)',\n",
    " 'PaymentMethod_Electronic check',\n",
    " 'PaymentMethod_Mailed check',\n",
    " 'Gender',\n",
    " 'Partner',\n",
    " 'Dependents',\n",
    " 'PhoneService',\n",
    " 'PaperlessBilling']\n",
    "# X = data_1[features1]\n",
    "# y = data_1.Churn \n",
    "\n",
    "\n",
    "\n",
    "features2 = ['age',\n",
    " 'fnlwgt',\n",
    " 'education-num',\n",
    " 'capital-gain',\n",
    " 'capital-loss',\n",
    " 'hours-per-week',\n",
    " 'workclass_Federal-gov',\n",
    " 'workclass_Local-gov',\n",
    " 'workclass_Never-worked',\n",
    " 'workclass_Private',\n",
    " 'workclass_Self-emp-inc',\n",
    " 'workclass_Self-emp-not-inc',\n",
    " 'workclass_State-gov',\n",
    " 'workclass_Without-pay',\n",
    " 'education_10th',\n",
    " 'education_11th',\n",
    " 'education_12th',\n",
    " 'education_1st-4th',\n",
    " 'education_5th-6th',\n",
    " 'education_7th-8th',\n",
    " 'education_9th',\n",
    " 'education_Assoc-acdm',\n",
    " 'education_Assoc-voc',\n",
    " 'education_Bachelors',\n",
    " 'education_Doctorate',\n",
    " 'education_HS-grad',\n",
    " 'education_Masters',\n",
    " 'education_Preschool',\n",
    " 'education_Prof-school',\n",
    " 'education_Some-college',\n",
    " 'marital-status_Divorced',\n",
    " 'marital-status_Married-AF-spouse',\n",
    " 'marital-status_Married-civ-spouse',\n",
    " 'marital-status_Married-spouse-absent',\n",
    " 'marital-status_Never-married',\n",
    " 'marital-status_Separated',\n",
    " 'marital-status_Widowed',\n",
    " 'occupation_Adm-clerical',\n",
    " 'occupation_Armed-Forces',\n",
    " 'occupation_Craft-repair',\n",
    " 'occupation_Exec-managerial',\n",
    " 'occupation_Farming-fishing',\n",
    " 'occupation_Handlers-cleaners',\n",
    " 'occupation_Machine-op-inspct',\n",
    " 'occupation_Other-service',\n",
    " 'occupation_Priv-house-serv',\n",
    " 'occupation_Prof-specialty',\n",
    " 'occupation_Protective-serv',\n",
    " 'occupation_Sales',\n",
    " 'occupation_Tech-support',\n",
    " 'occupation_Transport-moving',\n",
    " 'relationship_Husband',\n",
    " 'relationship_Not-in-family',\n",
    " 'relationship_Other-relative',\n",
    " 'relationship_Own-child',\n",
    " 'relationship_Unmarried',\n",
    " 'relationship_Wife',\n",
    " 'race_Amer-Indian-Eskimo',\n",
    " 'race_Asian-Pac-Islander',\n",
    " 'race_Black',\n",
    " 'race_Other',\n",
    " 'race_White',\n",
    " 'native-country_Cambodia',\n",
    " 'native-country_Canada',\n",
    " 'native-country_China',\n",
    " 'native-country_Columbia',\n",
    " 'native-country_Cuba',\n",
    " 'native-country_Dominican-Republic',\n",
    " 'native-country_Ecuador',\n",
    " 'native-country_El-Salvador',\n",
    " 'native-country_England',\n",
    " 'native-country_France',\n",
    " 'native-country_Germany',\n",
    " 'native-country_Greece',\n",
    " 'native-country_Guatemala',\n",
    " 'native-country_Haiti',\n",
    " 'native-country_Holand-Netherlands',\n",
    " 'native-country_Honduras',\n",
    " 'native-country_Hong',\n",
    " 'native-country_Hungary',\n",
    " 'native-country_India',\n",
    " 'native-country_Iran',\n",
    " 'native-country_Ireland',\n",
    " 'native-country_Italy',\n",
    " 'native-country_Jamaica',\n",
    " 'native-country_Japan',\n",
    " 'native-country_Laos',\n",
    " 'native-country_Mexico',\n",
    " 'native-country_Nicaragua',\n",
    " 'native-country_Outlying-US(Guam-USVI-etc)',\n",
    " 'native-country_Peru',\n",
    " 'native-country_Philippines',\n",
    " 'native-country_Poland',\n",
    " 'native-country_Portugal',\n",
    " 'native-country_Puerto-Rico',\n",
    " 'native-country_Scotland',\n",
    " 'native-country_South',\n",
    " 'native-country_Taiwan',\n",
    " 'native-country_Thailand',\n",
    " 'native-country_Trinadad&Tobago',\n",
    " 'native-country_United-States',\n",
    " 'native-country_Vietnam',\n",
    " 'native-country_Yugoslavia',\n",
    " 'sex']\n",
    "#X = data_2[features2]\n",
    "#y = data_2['>50K']\n",
    "\n",
    "\n",
    "#features3 = list(data_3.columns)[1:-1]\n",
    "#X = data_3[features3]\n",
    "#y = data_3.Class\n",
    "\n",
    "features4 = list(data_4.columns)[1:-1]\n",
    "X = data_4[features4]\n",
    "y = data_4['class']\n",
    "\n",
    "\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate = 0.001, n_iterations = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = 0\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, loss_threshold):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros((n_features,1))\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            linear = np.dot(X, self.weights) \n",
    "            y_hat = np.tanh(linear)\n",
    "            \n",
    "            y_predicted = self.predict(X)\n",
    "            accuracy = metrics.accuracy_score(y, y_predicted)\n",
    "            #print(accuracy)\n",
    "            \n",
    "            if (1-accuracy) < loss_threshold:\n",
    "                break\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (y - y_hat)*(1-y_hat**2))\n",
    "            self.weights +=  self.learning_rate * dw\n",
    "        return self.weights\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear = np.dot(X, self.weights)\n",
    "        y_hat = np.tanh(linear)\n",
    "        predicted_cls = [1 if i > 0 else -1 for i in y_hat]\n",
    "        return predicted_cls\n",
    "    \n",
    "    \n",
    "    def _tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "    \n",
    "        \n",
    "    def _mse(self, y, h_hat):\n",
    "        mse = np.sum((y - y_hat)**2, axis=1)/n_samples\n",
    "        return mse\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# For Logistic Regression Only\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "\n",
    "y_train = y_train.to_numpy().reshape((y_train.shape[0], 1))\n",
    "X_train = X_train.to_numpy()\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis = 1)\n",
    "\n",
    "y_test = y_test.to_numpy().reshape((y_test.shape[0], 1))\n",
    "X_test = X_test.to_numpy()\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis = 1)\n",
    "\n",
    "mylogreg = MyLogisticRegression()\n",
    "w = mylogreg.fit(X_train,y_train, 0.5)\n",
    "\n",
    "\n",
    "y_hat = mylogreg.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_hat)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Confussion MAtrix:\\n\", cnf_matrix)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_hat))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def adaBoost(X, y, L_weak, K):\n",
    "    examples = np.concatenate((X,y), axis = 1)\n",
    "    #print(\"ex shape\", examples.shape)\n",
    "    w = [] #np.ones(X.shape[0])*(1/X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        w.append(1/X.shape[0])\n",
    "    \n",
    "    h = []\n",
    "    z = np.zeros(K)\n",
    "    \n",
    "    #print(\"w\", len(w))\n",
    "    for k in range(K):\n",
    "        data = resample(examples, w)\n",
    "        X1 = data.iloc[ : , :-1]\n",
    "        y1 = data[data.columns[-1]]\n",
    "        \n",
    "        #print(X1.shape, y1.shape)\n",
    "        #X1 = X1.to_numpy()\n",
    "        #y1 = y1.to_numpy()\n",
    "        #print(X1.shape, y1.shape)\n",
    "        h.append(L_weak.fit(X1, y1, 0.5))\n",
    "        #print(\"h\", h)\n",
    "        error = 0\n",
    "        \n",
    "        for j in range(examples.shape[0]):\n",
    "            #print(\"last\", h[k])\n",
    "            y_hat = np.tanh(np.dot(X, h[k]))\n",
    "            predicted_cls = [1 if i > 0 else -1 for i in y_hat]\n",
    "            \n",
    "            if  predicted_cls[j] != y[j]:\n",
    "                error += w[j] \n",
    "            \n",
    "        if error > 0.5:\n",
    "            continue\n",
    "            \n",
    "        for j in range(examples.shape[0]):\n",
    "            y_hat = np.tanh(np.dot(X, h[k]))\n",
    "            predicted_cls = [1 if i > 0 else -1 for i in y_hat]\n",
    "            \n",
    "            if  predicted_cls[j] == y[j]:\n",
    "                w[j] = w[j]* error / (1 - error)\n",
    "        \n",
    "        for i in range(len(w)):\n",
    "            w[i] = w[i]/np.sum(w)\n",
    "        \n",
    "        z[k] = np.log2((1-error)/error)\n",
    "        \n",
    "        return (h,z) #Weighted_Majority(X, h, z)        \n",
    "        \n",
    "        \n",
    "        \n",
    "def resample(examples, w):\n",
    "    df = []\n",
    "    #print(\"Re\", examples.shape)\n",
    "    index = np.random.choice(examples.shape[0], examples.shape[0], p=w)\n",
    "    for i in index:\n",
    "        df.append(examples[i])\n",
    "    df = pd.DataFrame(df)\n",
    "    #print(df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def weighted_majority(X, h, z):\n",
    "    sum = 0\n",
    "    for i in range(len(h)):\n",
    "        sum += np.tanh(np.dot(X, h[i])) * z[i]\n",
    "    return sum\n",
    "    \n",
    "    \n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "\n",
    "y_train = y_train.to_numpy().reshape((y_train.shape[0], 1))\n",
    "X_train = X_train.to_numpy()\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis = 1)\n",
    "\n",
    "y_test = y_test.to_numpy().reshape((y_test.shape[0], 1))\n",
    "X_test = X_test.to_numpy()\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis = 1)\n",
    "\n",
    "\n",
    "# AdaBoost \n",
    "mylogreg = MyLogisticRegression()\n",
    "(h,z) = adaBoost( X_train, y_train, mylogreg, 10)\n",
    "\n",
    "\n",
    "y_hat = weighted_majority(X_test, h, z)\n",
    "predicted_cls = [1 if i > 0 else -1 for i in y_hat]\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, predicted_cls)\n",
    "print(\"AdaBoost:\")\n",
    "print(\"Confussion Matrix:\\n\", cnf_matrix)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, predicted_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33edd550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb251fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
